{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Deep Learning Practice (series 1)\n",
    "##### Mostafa Shahbazi Dill - id: 40252521602\n",
    "##### 2024-March-01\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import madgrad\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### 1. Dataset Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transform to convert images to tensors\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", train=True, transform=transform, download=True\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", train=False, transform=transform, download=True\n",
    ")\n",
    "\n",
    "# Define data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Compute the number of input nodes\n",
    "input_nodes = 28 * 28  # MNIST images are 28x28 pixels\n",
    "\n",
    "# Compute the number of output nodes\n",
    "output_nodes = 10  # Since there are 10 classes (digits 0-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.1\n",
    "##### MLP model with two hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MLP(nn.Module):\n",
    "#     def __init__(\n",
    "#         self, input_size, hidden_size1, hidden_size2, output_size, dropout_rate\n",
    "#     ):\n",
    "#         super(MLP, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "#         self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "#         self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.view(x.size(0), -1)  # Flatten the input images\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = torch.relu(self.fc2(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size1,\n",
    "        hidden_size2,\n",
    "        output_size,\n",
    "        dropout_rate,\n",
    "        activation,\n",
    "    ):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input images\n",
    "\n",
    "        # x = torch.relu(self.fc1(x)) # default activation function\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.activation(self.fc2(x))\n",
    "        # x = torch.relu(self.fc2(x)) # default activation function\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.2\n",
    "##### training loop and evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_loader, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        # print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    # print(f\"Accuracy on test set: {accuracy}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "#### 2.1\n",
    "##### optimization methods: SGD, Adam, RMSprop, MADGRAD (paper), mirrorMADGRAD (paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>(SGD)<<Accuracy: 0.9198\n",
      ">>(Adam)<<Accuracy: 0.9338\n",
      ">>(RMSprop)<<Accuracy: 0.9302\n",
      ">>(MADGRAD - paper op)<<Accuracy: 0.9275\n",
      ">>(MirrorMADGRAD - paper op)<<Accuracy: 0.9439\n"
     ]
    }
   ],
   "source": [
    "# Define the model, loss function, and optimization methods\n",
    "model = MLP(\n",
    "    input_size=input_nodes,\n",
    "    hidden_size1=128,\n",
    "    hidden_size2=64,\n",
    "    output_size=output_nodes,\n",
    "    dropout_rate=0.5,\n",
    "    activation=torch.relu,\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train with SGD optimizer\n",
    "optimizer_sgd = optim.SGD(model.parameters(), lr=0.01)\n",
    "train(model, criterion, optimizer_sgd, train_loader, epochs=5)\n",
    "accuracy_sgd = evaluate(model, test_loader)\n",
    "\n",
    "# Train with Adam optimizer\n",
    "model = MLP(\n",
    "    input_size=input_nodes,\n",
    "    hidden_size1=128,\n",
    "    hidden_size2=64,\n",
    "    output_size=output_nodes,\n",
    "    dropout_rate=0.5,\n",
    "    activation=torch.relu,\n",
    ")\n",
    "optimizer_adam = optim.Adam(model.parameters(), lr=0.001)\n",
    "train(model, criterion, optimizer_adam, train_loader, epochs=5)\n",
    "accuracy_adam = evaluate(model, test_loader)\n",
    "\n",
    "# Train with RMSprop optimizer\n",
    "model = MLP(\n",
    "    input_size=input_nodes,\n",
    "    hidden_size1=128,\n",
    "    hidden_size2=64,\n",
    "    output_size=output_nodes,\n",
    "    dropout_rate=0.5,\n",
    "    activation=torch.relu,\n",
    ")\n",
    "optimizer_rmsprop = optim.RMSprop(model.parameters(), lr=0.001)\n",
    "train(model, criterion, optimizer_rmsprop, train_loader, epochs=5)\n",
    "accuracy_rmsprop = evaluate(model, test_loader)\n",
    "\n",
    "# Train with MADGRAD optimizer (our new optimizer according to the paper)\n",
    "model = MLP(\n",
    "    input_size=input_nodes,\n",
    "    hidden_size1=128,\n",
    "    hidden_size2=64,\n",
    "    output_size=output_nodes,\n",
    "    dropout_rate=0.5,\n",
    "    activation=torch.relu,\n",
    ")\n",
    "optimizer_madgrad = madgrad.MADGRAD(\n",
    "    params=model.parameters(), decouple_decay=True, lr=0.001\n",
    ")\n",
    "train(model, criterion, optimizer_madgrad, train_loader, epochs=5)\n",
    "accuracy_madgrad = evaluate(model, test_loader)\n",
    "\n",
    "# Train with MirrorMADGRAD optimizer\n",
    "model = MLP(\n",
    "    input_size=input_nodes,\n",
    "    hidden_size1=128,\n",
    "    hidden_size2=64,\n",
    "    output_size=output_nodes,\n",
    "    dropout_rate=0.5,\n",
    "    activation=torch.relu,\n",
    ")\n",
    "optimizer_mirrormadgrad = madgrad.MirrorMADGRAD(\n",
    "    params=model.parameters(), decouple_decay=True, lr=0.001\n",
    ")\n",
    "train(model, criterion, optimizer_mirrormadgrad, train_loader, epochs=5)\n",
    "accuracy_mirrormadgrad = evaluate(model, test_loader)\n",
    "\n",
    "\n",
    "print(f\">>(SGD)<<Accuracy: {accuracy_sgd}\")\n",
    "print(f\">>(Adam)<<Accuracy: {accuracy_adam}\")\n",
    "print(f\">>(RMSprop)<<Accuracy: {accuracy_rmsprop}\")\n",
    "print(f\">>(MADGRAD - paper op)<<Accuracy: {accuracy_madgrad}\")\n",
    "print(f\">>(MirrorMADGRAD - paper op)<<Accuracy: {accuracy_mirrormadgrad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### 3.1\n",
    "##### evaluate with different activation functions\n",
    "\n",
    "##### activation functions: ReLU, Sigmoid, Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with activation relu: 0.9303\n",
      "Accuracy with activation sigmoid: 0.9428\n",
      "Accuracy with activation tanh: 0.9181\n",
      "\n",
      "Optimizer: MADGRAD\n"
     ]
    }
   ],
   "source": [
    "# Define activation functions\n",
    "activations = [torch.relu, torch.sigmoid, torch.tanh]\n",
    "\n",
    "# Evaluate the model with different activation functions\n",
    "for activation in activations:\n",
    "    model = MLP(\n",
    "        input_nodes, 128, 64, output_nodes, dropout_rate=0.5, activation=activation\n",
    "    )\n",
    "\n",
    "    optimizer = madgrad.MADGRAD(params=model.parameters(), lr=0.001)\n",
    "    train(model, criterion, optimizer, train_loader, epochs=5)\n",
    "    accuracy = evaluate(model, test_loader)\n",
    "    print(f\"Accuracy with activation {activation.__name__}: {accuracy}\")\n",
    "\n",
    "print(f\"\\nOptimizer: {optimizer.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## 4.1\n",
    "#### Dropout rate: 0.2, 0.5, 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with dropout rate 0.1: 0.9651\n",
      "Accuracy with dropout rate 0.3: 0.9609\n",
      "Accuracy with dropout rate 0.5: 0.9417\n",
      "Accuracy with dropout rate 0.7: 0.9105\n",
      "Accuracy with dropout rate 0.9: 0.5874\n",
      "\n",
      "Optimizer: MADGRAD\n"
     ]
    }
   ],
   "source": [
    "dropout_rates = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "for dropout_rate in dropout_rates:\n",
    "    model = MLP(\n",
    "        input_size=input_nodes,\n",
    "        hidden_size1=128,\n",
    "        hidden_size2=64,\n",
    "        output_size=output_nodes,\n",
    "        dropout_rate=dropout_rate,\n",
    "        activation=torch.sigmoid,\n",
    "    )\n",
    "\n",
    "    optimizer = madgrad.MADGRAD(params=model.parameters(), lr=0.001)\n",
    "    train(model, criterion, optimizer, train_loader, epochs=5)\n",
    "    accuracy = evaluate(model, test_loader)\n",
    "    print(f\"Accuracy with dropout rate {dropout_rate}: {accuracy}\")\n",
    "\n",
    "print(f\"\\nOptimizer: {optimizer.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### 5.1\n",
    "#### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with data augmentation: 0.8343\n"
     ]
    }
   ],
   "source": [
    "# Augmentation transforms\n",
    "augmentation_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Augmented dataset\n",
    "augmented_train_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", train=True, transform=augmentation_transform, download=True\n",
    ")\n",
    "augmented_train_loader = DataLoader(\n",
    "    augmented_train_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "\n",
    "# Evaluate the model with augmented data\n",
    "model = MLP(\n",
    "    input_size=input_nodes,\n",
    "    hidden_size1=128,\n",
    "    hidden_size2=64,\n",
    "    output_size=output_nodes,\n",
    "    dropout_rate=0.1,\n",
    "    activation=torch.sigmoid,\n",
    ")\n",
    "optimizer = madgrad.MirrorMADGRAD(params=model.parameters(), lr=0.001)\n",
    "# optimizer = optim.Adam(params=model.parameters(), lr=0.001)\n",
    "train(model, criterion, optimizer, augmented_train_loader, epochs=5)\n",
    "accuracy_augmented = evaluate(model, test_loader)\n",
    "print(f\"Accuracy with data augmentation: {accuracy_augmented}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1\n",
    "#### Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_plt(model, criterion, optimizer, train_loader, epochs, batch_size):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()  # * inputs.size(0)  # Multiply by batch size\n",
    "            # if i % 100 == 0:\n",
    "            #     print(\n",
    "            #         f\"Epoch {epoch+1}, Batch {i}, Loss: {running_loss/(i*batch_size)}\"\n",
    "            #     )\n",
    "        epoch_loss = running_loss / len(\n",
    "            train_loader.dataset\n",
    "        )  # Divide by total number of samples\n",
    "        losses.append(epoch_loss)\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    class MLP(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size1, hidden_size2, output_size, dropout_rate):\n",
    "            super(MLP, self).__init__()\n",
    "            self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "            self.bn1 = nn.BatchNorm1d(hidden_size1)\n",
    "            self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "            self.bn2 = nn.BatchNorm1d(hidden_size2)\n",
    "            self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "            self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.view(x.size(0), -1)  # Flatten the input images\n",
    "            x = self.bn1(torch.relu(self.fc1(x)))\n",
    "            x = self.dropout(x)\n",
    "            x = self.bn2(torch.relu(self.fc2(x)))\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc3(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with batch size: 16\n"
     ]
    }
   ],
   "source": [
    "# Define batch sizes to test\n",
    "batch_sizes = [16, 128]\n",
    "nn.BatchNorm1d()\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"\\nTraining with batch size: {batch_size}\")\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    model = MLP(\n",
    "        input_size=input_nodes,\n",
    "        hidden_size1=128,\n",
    "        hidden_size2=64,\n",
    "        output_size=output_nodes,\n",
    "        dropout_rate=0.1,\n",
    "        activation=torch.sigmoid,\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    train_plt(\n",
    "        model, criterion, optimizer, train_loader, epochs=5, batch_size=batch_size\n",
    "    )\n",
    "    accuracy = evaluate(model, test_loader)\n",
    "    print(f\"Accuracy with batch size {batch_size}: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
