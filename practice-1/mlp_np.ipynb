{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class of a NeuralNetwork with SGD, Momentum and AdaGrad\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "\n",
    "plt.matplotlib.use(\"Agg\")\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(\n",
    "        self,\n",
    "        samples,\n",
    "        labels,\n",
    "        size_hidden=30,\n",
    "        eta=0.1,\n",
    "        my=0.9,\n",
    "        epochs=10,\n",
    "        optimizer=\"sgd\",\n",
    "        verbose=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        :param samples: input samples\n",
    "        :param labels: input labels\n",
    "        :param size_hidden: number of units in hidden layer\n",
    "        :param eta: learning rate\n",
    "        :param my: learning factor for momentum\n",
    "        :param epochs: number of epochs\n",
    "        :param optimizer: type of optimizer (\"sgd\", \"momentum\", \"adagrad\")\n",
    "        :param verbose: print accuracy and error per epoch\n",
    "        \"\"\"\n",
    "        self.samples = samples\n",
    "        self.labels = labels\n",
    "        self.w01 = np.random.random((len(self.samples[0]), size_hidden))\n",
    "        self.w12 = np.random.random((size_hidden, len(self.labels[0])))\n",
    "        self.v01 = np.zeros((len(self.samples[0]), size_hidden))\n",
    "        self.v12 = np.zeros((size_hidden, len(self.labels[0])))\n",
    "        self.g01 = np.zeros((len(self.samples[0]), size_hidden))\n",
    "        self.g12 = np.zeros((size_hidden, len(self.labels[0])))\n",
    "        self.b1 = np.array([0])\n",
    "        self.b2 = np.array([0])\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "        self.my = my\n",
    "        self.optimizer = optimizer\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def sigmoid(self, x, deriv=False):\n",
    "        if deriv == True:\n",
    "            return x * (1 - x)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x, deriv=False):\n",
    "        if deriv == True:\n",
    "            # Return the partial derivation of the activation function\n",
    "            return np.multiply(x, 1 - x)\n",
    "        y = x - np.max(x)\n",
    "        e_x = np.exp(y)\n",
    "        return e_x / e_x.sum()\n",
    "\n",
    "    def relu(self, x, deriv=False):\n",
    "        if deriv == True:\n",
    "            return 1.0 * (x > 0)\n",
    "        return x * (x > 0)\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Method to fit the input data and optimize the weights in the neural network\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        accuracy = []\n",
    "        no_epochs = []\n",
    "        sample_no = 0\n",
    "\n",
    "        if self.optimizer == \"adagrad\":\n",
    "            # initialize matrix for adagrad\n",
    "            gti_01 = np.zeros(len(self.w01[0]))\n",
    "            gti_12 = np.zeros(len(self.w12[0]))\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range(0, len(self.samples), 1):\n",
    "                sample_no += 1\n",
    "                l0 = self.samples[i : i + 1]\n",
    "                y = self.labels[i : i + 1]\n",
    "\n",
    "                # Feed Forward Pass\n",
    "                l1 = self.relu(np.dot(l0, self.w01) + 1 * self.b1)\n",
    "                l2 = self.softmax(np.dot(l1, self.w12) + 1 * self.b2)\n",
    "\n",
    "                l2_error = (1 / 2) * np.power((y - l2), 2)\n",
    "                l2_error_total = str(np.mean(np.abs(l2_error)))\n",
    "\n",
    "                if l2_error_total == 1.0:\n",
    "                    if self.verbose:\n",
    "                        print(\"Overflow\")\n",
    "                    return\n",
    "                # Backpropagation\n",
    "                # dE_total/douto\n",
    "                l2_delta = -1 * (y - l2)\n",
    "                # douto/dneto = deriv activation\n",
    "                l2_delta = l2_delta * self.softmax(l2, deriv=True)\n",
    "                # dneth/dw\n",
    "                l2_delta = np.dot(l2_delta.T, l1)\n",
    "\n",
    "                # dEo/neto\n",
    "                # dEo/douto * douto/dneto\n",
    "                l1_delta = np.sum(\n",
    "                    ((-1 * (y - l2)) * self.softmax(l2, deriv=True)), axis=0\n",
    "                )\n",
    "                # dEo/outh\n",
    "                # dEo/neto * dneto/douth\n",
    "                l1_delta = l1_delta * self.w12\n",
    "\n",
    "                # dEtotal/outh = Sum(Eo/outh)\n",
    "                l1_delta = np.sum(l1_delta, axis=1)\n",
    "                # douth/neth\n",
    "                l1_delta = l1_delta * self.relu(l1, deriv=True)\n",
    "                # dneth/dw\n",
    "                l1_delta = np.dot(l1_delta.T, l0)\n",
    "\n",
    "                if self.optimizer == \"adagrad\":\n",
    "                    # Fundamental idea using https://xcorr.net/2014/01/23/adagrad-eliminating-learning-rates-in-stochastic-gradient-descent/\n",
    "                    # Update Weights using AdaGrad\n",
    "                    grad_12 = self.eta * l2_delta.T\n",
    "                    self.g12 += np.power(grad_12, 2)\n",
    "                    adjusted_grad = grad_12 / np.sqrt(0.0000001 + self.g12)\n",
    "                    self.w12 = self.w12 - adjusted_grad\n",
    "\n",
    "                    grad_01 = self.eta * l1_delta.T\n",
    "                    self.g01 += np.power(grad_01, 2)\n",
    "                    adjusted_grad = grad_01 / np.sqrt(0.0000001 + self.g01)\n",
    "                    self.w01 = self.w01 - adjusted_grad\n",
    "\n",
    "                if self.optimizer == \"sgd\":\n",
    "                    # Update Weights\n",
    "                    self.w01 -= (self.eta / ((epoch + 1) / 50)) * l1_delta.T\n",
    "                    self.w12 -= (self.eta / ((epoch + 1) / 50)) * l2_delta.T\n",
    "\n",
    "                if self.optimizer == \"momentum\":\n",
    "                    # Update Weights using Momentum\n",
    "                    self.v01 = self.my * self.v01 + self.eta * l1_delta.T\n",
    "                    self.w01 -= self.v01\n",
    "                    self.v12 = self.my * self.v12 + self.eta * l2_delta.T\n",
    "                    self.w12 -= self.v12\n",
    "\n",
    "            if epoch % 1 == 0:\n",
    "                if self.verbose:\n",
    "                    y_pred, y_true = self.predict(X_test, y_test)\n",
    "                    acc = accuracy_score(y_true, y_pred)\n",
    "                    print(\n",
    "                        \"Epoch: \",\n",
    "                        epoch,\n",
    "                        \" - Error: \",\n",
    "                        l2_error_total,\n",
    "                        \" - Accuracy im Testset: \",\n",
    "                        acc,\n",
    "                    )\n",
    "                    y_pred, y_true = self.predict(X_train, y_train)\n",
    "                    print(\n",
    "                        \"Epoch: \",\n",
    "                        epoch,\n",
    "                        \" - Error: \",\n",
    "                        l2_error_total,\n",
    "                        \" - Accuracy im Trainingsset: \",\n",
    "                        accuracy_score(y_true, y_pred),\n",
    "                    )\n",
    "                    print(\"############################################\")\n",
    "\n",
    "                    accuracy.append(acc)\n",
    "                    no_epochs.append(sample_no)\n",
    "        if self.verbose:\n",
    "            return no_epochs, accuracy\n",
    "\n",
    "    def predict(self, test_samples, test_labels):\n",
    "        \"\"\"\n",
    "        Predict test data using the fitted model\n",
    "        :param test_samples:\n",
    "        :param test_labels:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        l1 = self.relu(np.dot(test_samples, self.w01) + 1 * self.b1)\n",
    "        l2 = self.softmax(np.dot(l1, self.w12) + 1 * self.b2)\n",
    "        y_pred = (l2 == l2.max(axis=1)[:, None]).astype(float)\n",
    "        res_pred = []\n",
    "        res_labels = []\n",
    "\n",
    "        def checkEqual1(iterator):\n",
    "            iterator = iter(iterator)\n",
    "            try:\n",
    "                first = next(iterator)\n",
    "            except StopIteration:\n",
    "                return True\n",
    "            return all(first == rest for rest in iterator)\n",
    "\n",
    "        for k in y_pred:\n",
    "            for i, j in enumerate(k):\n",
    "                if int(j) == 1 and not checkEqual1(k):\n",
    "                    res_pred.append(i)\n",
    "                    break\n",
    "                if checkEqual1(k):\n",
    "                    res_pred.append(0)\n",
    "                    break\n",
    "        for k in test_labels:\n",
    "            for i, j in enumerate(k):\n",
    "                if j == 1.0:\n",
    "                    res_labels.append(i)\n",
    "\n",
    "        return res_pred, res_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:153\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:182\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m eta \u001b[38;5;129;01min\u001b[39;00m etas:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(eta)\n\u001b[0;32m---> 33\u001b[0m     NN \u001b[38;5;241m=\u001b[39m \u001b[43mNeuralNetwork\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43msamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43msize_hidden\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msgd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     fitted \u001b[38;5;241m=\u001b[39m NN\u001b[38;5;241m.\u001b[39mfit()\n\u001b[1;32m     43\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(fitted[\u001b[38;5;241m0\u001b[39m], fitted[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my-\u001b[39m\u001b[38;5;124m\"\u001b[39m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msgd; Eta=3.5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 26\u001b[0m, in \u001b[0;36mNeuralNetwork.__init__\u001b[0;34m(self, samples, labels, size_hidden, eta, my, epochs, optimizer, verbose)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples \u001b[38;5;241m=\u001b[39m samples\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m labels\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw01 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m), size_hidden))\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw12 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom((size_hidden, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[\u001b[38;5;241m0\u001b[39m])))\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv01 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[\u001b[38;5;241m0\u001b[39m]), size_hidden))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3809\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3805\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3806\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3807\u001b[0m     ):\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3809\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3811\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3812\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# Prepare dataset and split into test and training data\n",
    "\"\"\"\n",
    "# Digits\n",
    "digits = load_digits()\n",
    "samples = digits.data\n",
    "y = digits.target.reshape((len(samples),1))\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(y)\n",
    "labels = enc.transform(y).toarray()\n",
    "X_train, X_test, y_train, y_test = train_test_split(samples, labels, test_size=0.33, random_state=42)\n",
    "\"\"\"\n",
    "# MNIST\n",
    "# mnist = fetch_openml(\"mnist_784\", data_home=\"./data\")\n",
    "# MNIST\n",
    "mnist = fetch_openml(\"mnist_784\", data_home=\"./data\")\n",
    "samples = mnist[\"data\"]\n",
    "samples = samples / (len(samples) * 10)\n",
    "y = np.array(mnist[\"target\"]).reshape((len(samples), 1))\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(y)\n",
    "labels = enc.transform(y).toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    samples, labels, test_size=0.33, random_state=42\n",
    ")\n",
    "# Create instance of NeuralNetwork, fit to dataset, predict and print accuracy\n",
    "\n",
    "etas = [3.5]\n",
    "\n",
    "for eta in etas:\n",
    "    print(eta)\n",
    "    NN = NeuralNetwork(\n",
    "        samples=X_train,\n",
    "        labels=y_train,\n",
    "        eta=eta,\n",
    "        epochs=50,\n",
    "        size_hidden=40,\n",
    "        optimizer=\"sgd\",\n",
    "        verbose=True,\n",
    "    )\n",
    "    fitted = NN.fit()\n",
    "    plt.plot(fitted[0], fitted[1], \"y-\", linewidth=2, label=\"sgd; Eta=3.5\")\n",
    "\n",
    "    NN = NeuralNetwork(\n",
    "        samples=X_train,\n",
    "        labels=y_train,\n",
    "        eta=eta,\n",
    "        epochs=50,\n",
    "        size_hidden=40,\n",
    "        optimizer=\"momentum\",\n",
    "        verbose=True,\n",
    "    )\n",
    "    fitted = NN.fit()\n",
    "    plt.plot(fitted[0], fitted[1], \"b-\", linewidth=2, label=\"momentum; Eta=0.1, My=0.9\")\n",
    "\n",
    "    NN = NeuralNetwork(\n",
    "        samples=X_train,\n",
    "        labels=y_train,\n",
    "        eta=1,\n",
    "        epochs=50,\n",
    "        size_hidden=40,\n",
    "        optimizer=\"adagrad\",\n",
    "        verbose=True,\n",
    "    )\n",
    "    fitted = NN.fit()\n",
    "    plt.plot(fitted[0], fitted[1], \"r-\", linewidth=2, label=\"adagrad; Eta=1\")\n",
    "\n",
    "    plt.xlabel(\"Samples seen\")\n",
    "    plt.ylabel(\"Accuarcy\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    fig = plt.gcf()\n",
    "    fig.savefig(\"eval_nn.png\")\n",
    "\n",
    "    # y_pred, y_true = NN.predict(X_test, y_test)\n",
    "\n",
    "    # print(\"Accuracy: \",accuracy_score(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
