{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Deep Learning Practice (series 1)\n",
    "##### Mostafa Shahbazi Dill - id: 40252521602\n",
    "##### 2024-March-01\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import madgrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### 1. Dataset Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transform to convert images to tensors\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", train=True, transform=transform, download=True\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", train=False, transform=transform, download=True\n",
    ")\n",
    "\n",
    "# Define data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Compute the number of input nodes\n",
    "input_nodes = 28 * 28  # MNIST images are 28x28 pixels\n",
    "\n",
    "# Compute the number of output nodes\n",
    "output_nodes = 10  # Since there are 10 classes (digits 0-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.1\n",
    "##### MLP model with two hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MLP(nn.Module):\n",
    "#     def __init__(\n",
    "#         self, input_size, hidden_size1, hidden_size2, output_size, dropout_rate\n",
    "#     ):\n",
    "#         super(MLP, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "#         self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "#         self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.view(x.size(0), -1)  # Flatten the input images\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = torch.relu(self.fc2(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size1,\n",
    "        hidden_size2,\n",
    "        output_size,\n",
    "        dropout_rate,\n",
    "        activation,\n",
    "    ):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input images\n",
    "\n",
    "        # x = torch.relu(self.fc1(x)) # default activation function\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.activation(self.fc2(x))\n",
    "        # x = torch.relu(self.fc2(x)) # default activation function\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.2\n",
    "##### training loop and evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_loader, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        # print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    # print(f\"Accuracy on test set: {accuracy}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "#### 2.1\n",
    "##### optimization methods: SGD, Adam, RMSprop, MADGRAD (paper), mirrorMADGRAD (paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>(SGD)<<Accuracy: 0.9186\n",
      ">>(Adam)<<Accuracy: 0.9397\n",
      ">>(RMSprop)<<Accuracy: 0.9349\n",
      "==(MADGRAD - paper op)==Accuracy: 0.9247\n",
      "--(MirrorMADGRAD - paper op)--Accuracy: 0.9399\n"
     ]
    }
   ],
   "source": [
    "# Define the model, loss function, and optimization methods\n",
    "model = MLP(\n",
    "    input_size=input_nodes,\n",
    "    hidden_size1=128,\n",
    "    hidden_size2=64,\n",
    "    output_size=output_nodes,\n",
    "    dropout_rate=0.5,\n",
    "    activation=torch.relu,\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train with SGD optimizer\n",
    "optimizer_sgd = optim.SGD(model.parameters(), lr=0.01)\n",
    "train(model, criterion, optimizer_sgd, train_loader, epochs=5)\n",
    "accuracy_sgd = evaluate(model, test_loader)\n",
    "\n",
    "# Train with Adam optimizer\n",
    "model = MLP(\n",
    "    input_size=input_nodes,\n",
    "    hidden_size1=128,\n",
    "    hidden_size2=64,\n",
    "    output_size=output_nodes,\n",
    "    dropout_rate=0.5,\n",
    "    activation=torch.relu,\n",
    ")\n",
    "optimizer_adam = optim.Adam(model.parameters(), lr=0.001)\n",
    "train(model, criterion, optimizer_adam, train_loader, epochs=5)\n",
    "accuracy_adam = evaluate(model, test_loader)\n",
    "\n",
    "# Train with RMSprop optimizer\n",
    "model = MLP(\n",
    "    input_size=input_nodes,\n",
    "    hidden_size1=128,\n",
    "    hidden_size2=64,\n",
    "    output_size=output_nodes,\n",
    "    dropout_rate=0.5,\n",
    "    activation=torch.relu,\n",
    ")\n",
    "optimizer_rmsprop = optim.RMSprop(model.parameters(), lr=0.001)\n",
    "train(model, criterion, optimizer_rmsprop, train_loader, epochs=5)\n",
    "accuracy_rmsprop = evaluate(model, test_loader)\n",
    "\n",
    "# Train with MADGRAD optimizer (our new optimizer according to the paper)\n",
    "model = MLP(\n",
    "    input_size=input_nodes,\n",
    "    hidden_size1=128,\n",
    "    hidden_size2=64,\n",
    "    output_size=output_nodes,\n",
    "    dropout_rate=0.5,\n",
    "    activation=torch.relu,\n",
    ")\n",
    "optimizer_madgrad = madgrad.MADGRAD(\n",
    "    params=model.parameters(), decouple_decay=True, lr=0.001\n",
    ")\n",
    "train(model, criterion, optimizer_madgrad, train_loader, epochs=5)\n",
    "accuracy_madgrad = evaluate(model, test_loader)\n",
    "\n",
    "# Train with MirrorMADGRAD optimizer\n",
    "model = MLP(\n",
    "    input_size=input_nodes,\n",
    "    hidden_size1=128,\n",
    "    hidden_size2=64,\n",
    "    output_size=output_nodes,\n",
    "    dropout_rate=0.5,\n",
    "    activation=torch.relu,\n",
    ")\n",
    "optimizer_mirrormadgrad = madgrad.MirrorMADGRAD(\n",
    "    params=model.parameters(), decouple_decay=True, lr=0.001\n",
    ")\n",
    "train(model, criterion, optimizer_mirrormadgrad, train_loader, epochs=5)\n",
    "accuracy_mirrormadgrad = evaluate(model, test_loader)\n",
    "\n",
    "\n",
    "print(f\">>(SGD)<<Accuracy: {accuracy_sgd}\")\n",
    "print(f\">>(Adam)<<Accuracy: {accuracy_adam}\")\n",
    "print(f\">>(RMSprop)<<Accuracy: {accuracy_rmsprop}\")\n",
    "print(f\">>(MADGRAD - paper op)<<Accuracy: {accuracy_madgrad}\")\n",
    "print(f\">>(MirrorMADGRAD - paper op)<<Accuracy: {accuracy_mirrormadgrad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### 3.1\n",
    "##### evaluate different activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size1,\n",
    "        hidden_size2,\n",
    "        output_size,\n",
    "        dropout_rate=None,\n",
    "        activation=None,\n",
    "    ):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        if dropout_rate: self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input images\n",
    "        if self.activation: x = self.activation(self.fc1(x))\n",
    "        if self.dropout: x = self.dropout(x)\n",
    "        if self.activation: x = self.activation(self.fc2(x))\n",
    "        if self.dropout: x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.2\n",
    "##### activation functions: ReLU, Sigmoid, Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.9332\n",
      "Accuracy with relu: 0.9332\n",
      "Accuracy on test set: 0.9434\n",
      "Accuracy with sigmoid: 0.9434\n",
      "Accuracy on test set: 0.9403\n",
      "Accuracy with tanh: 0.9403\n"
     ]
    }
   ],
   "source": [
    "# Define activation functions\n",
    "activations = [torch.relu, torch.sigmoid, torch.softmax]\n",
    "\n",
    "# Evaluate the model with different activation functions\n",
    "for activation in activations:\n",
    "    model = MLP(\n",
    "        input_nodes, 128, 64, output_nodes, dropout_rate=0.5, activation=activation\n",
    "    )\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    train(model, criterion, optimizer, train_loader, epochs=5)\n",
    "    accuracy = evaluate(model, test_loader)\n",
    "    print(f\"Accuracy with {activation.__name__}: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.0567214161729508\n",
      "Epoch 2, Loss: 0.7946562373688989\n",
      "Epoch 3, Loss: 0.7316161442095283\n",
      "Epoch 4, Loss: 0.7116706320154134\n",
      "Epoch 5, Loss: 0.6789353833650984\n",
      "Accuracy on test set: 0.8722\n",
      "Accuracy with data augmentation: 0.8722\n"
     ]
    }
   ],
   "source": [
    "# Augmentation transforms\n",
    "augmentation_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Augmented dataset\n",
    "augmented_train_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", train=True, transform=augmentation_transform, download=True\n",
    ")\n",
    "augmented_train_loader = DataLoader(\n",
    "    augmented_train_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "\n",
    "# Evaluate the model with augmented data\n",
    "model = MLP(input_nodes, 128, 64, output_nodes, dropout_rate=0.5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "train(model, criterion, optimizer, augmented_train_loader, epochs=5)\n",
    "accuracy_augmented = evaluate(model, test_loader)\n",
    "print(f\"Accuracy with data augmentation: {accuracy_augmented}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
